# OpenAI Codex Configuration
# This file uses TOML format as required by Codex CLI
# Documentation: https://github.com/openai/codex/blob/main/codex-rs/config.md

# Default model configuration
# Model is specified via command line alias (-m gpt-5)
temperature = 0.7
max_tokens = 8192  # Match Claude Code's max output tokens for fair comparison

# Sandbox and permissions
sandbox = "local"
autonomy = "medium"
interactive = false

# Authentication (subscription-based)
# Use 'codex login' to authenticate with ChatGPT Plus/Pro/Team subscription

# Logging configuration
log_level = "info"
log_file = "~/.codex/logs/codex.log"

# MCP Server configurations
# Note: Codex uses 'mcp_servers' (underscore) not 'mcpServers' (camelCase)

[mcp_servers.playwright]
command = "npx"
args = ["@playwright/mcp@latest"]
env = { FASTMCP_LOG_LEVEL = "ERROR" }

# Additional MCP servers can be added here
# Example format:
# [mcp_servers.github]
# command = "npx"
# args = ["-y", "@modelcontextprotocol/server-github"]
# env = { GITHUB_TOKEN = "$GITHUB_TOKEN" }

# Fallback models for resilience
fallback_models = [
    "gpt-4-turbo",
    "gpt-4",
    "gpt-3.5-turbo"
]

# Integration settings
[integrations]
git_enabled = true
git_auto_commit = false

# Permissions configuration
[permissions]
allow = ["read", "write", "execute", "mcp"]
deny = ["network:external", "system:critical"]

# -----------------------------------------------------------------------------
# Model providers and profiles (per Codex docs)
# Docs: https://github.com/openai/codex/blob/main/docs/config.md
# -----------------------------------------------------------------------------

# Define an OpenAI Chat Completions provider and a profile for GPTâ€‘4o.
# Invoke with: codex --profile gpt4o "..."

[model_providers.openai-chat-completions]
name = "OpenAI using Chat Completions"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
wire_api = "chat"

[profiles.gpt4o]
model = "gpt-4o"
model_provider = "openai-chat-completions"
