# Step 4: Choose Your Use Case

## LLM Inference (Offline Chatbot)

This configuration sets up your Pi as a local inference server for small large-language models using [llama.cpp](https://github.com/ggerganov/llama.cpp). Perfect for offline agents, experimentation, or privacy-focused usage.

### Requirements
- Raspberry Pi 5 (8GB recommended)
- Raspberry Pi OS Lite (64-bit)
- Internet access for setup

