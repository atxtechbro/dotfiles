#!/bin/bash
# Convert provider-specific AI context files to AI-RULES.md with symlinks pattern
# This implements the standardized AI context file structure
# Handles: AmazonQ.md, CLAUDE.md, or any other AI provider files

set -euo pipefail

# Source logging framework (global after setup.sh)
if [[ -f ~/bin/logging.sh ]]; then
    source ~/bin/logging.sh
else
    # Fallback to relative path
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    source "${SCRIPT_DIR}/../utils/logging.sh"
fi

# Function to process a single repository
process_repo() {
    local repo_path="$1"
    local repo_name=$(basename "$repo_path")
    
    log_info "Processing: $repo_path"
    
    cd "$repo_path"
    
    # Check if it's a git repository
    if [ ! -d .git ]; then
        log_error "Not a git repository, skipping"
        return 1
    fi
    
    # Find which AI context file exists (AmazonQ.md, CLAUDE.md, etc.)
    local ai_file=""
    if [ -f "AmazonQ.md" ] && [ ! -L "AmazonQ.md" ]; then
        ai_file="AmazonQ.md"
    elif [ -f "CLAUDE.md" ] && [ ! -L "CLAUDE.md" ]; then
        ai_file="CLAUDE.md"
    else
        # Check if already converted
        if [ -f "AI-RULES.md" ] && ([ -L "AmazonQ.md" ] || [ -L "CLAUDE.md" ]); then
            log_success "Already converted"
            return 0
        fi
        log_info "No AI context file found (AmazonQ.md or CLAUDE.md), skipping"
        return 0
    fi
    
    # Perform conversion
    log_info "Converting $ai_file to AI-RULES.md..."
    
    # Move the provider-specific file to AI-RULES.md
    git mv "$ai_file" AI-RULES.md
    
    # Create symlink for the original file
    ln -s AI-RULES.md "$ai_file"
    git add "$ai_file"
    
    # Create symlinks for both standard names if they don't exist
    if [ ! -e "AmazonQ.md" ]; then
        ln -s AI-RULES.md AmazonQ.md
        git add AmazonQ.md
    fi
    
    # Create symlink: CLAUDE.md -> AI-RULES.md (if it doesn't exist)
    if [ ! -e "CLAUDE.md" ]; then
        ln -s AI-RULES.md CLAUDE.md
        git add CLAUDE.md
    elif [ ! -L "CLAUDE.md" ]; then
        log_warning "CLAUDE.md exists but is not a symlink, manual intervention needed"
    fi
    
    # Commit changes
    if git diff --cached --quiet; then
        log_info "No changes to commit"
    else
        git commit -m "refactor: standardize AI context files to AI-RULES.md pattern

- Convert $ai_file to AI-RULES.md
- Create symlinks for both AmazonQ.md and CLAUDE.md
- Provider-agnostic: works with any AI context file

This follows the dotfiles repository pattern for AI context management.

Principle: systems-stewardship"
        
        # Push to remote
        log_info "Pushing to remote..."
        git push
        
        log_success "Successfully converted and pushed"
    fi
    
    return 0
}

# Main execution
main() {
    echo "ðŸ”„ AI Rules File Converter"
    echo "=========================="
    echo
    echo "This script converts provider-specific AI context files to the standardized"
    echo "AI-RULES.md pattern with symlinks for both AmazonQ.md and CLAUDE.md."
    echo "Works with: AmazonQ.md, CLAUDE.md, or any existing AI provider file."
    echo
    
    # Parse arguments
    if [ $# -eq 0 ]; then
        echo "Usage: $0 <repo-path> [repo-path ...]"
        echo "   or: $0 --find-all"
        echo
        echo "Examples:"
        echo "  $0 ~/work/my-project"
        echo "  $0 ~/work/project1 ~/work/project2"
        echo "  $0 --find-all    # Find and process all repos with AmazonQ.md"
        exit 1
    fi
    
    local repos=()
    
    if [ "$1" == "--find-all" ]; then
        log_info "Finding all repositories with AI context files..."
        
        # Fast path: check common project directories first
        local common_dirs=(
            ~/work
            ~/projects
            ~/dev
            ~/code
            ~/src
            ~/ppv
            ~/repos
            ~/github
            ~/gitlab
        )
        
        # Function to search a directory
        search_directory() {
            local search_dir="$1"
            find "$search_dir" \
                -path "*/.*" -prune -o \
                -path "*/node_modules" -prune -o \
                -path "*/venv" -prune -o \
                -path "*/.venv" -prune -o \
                -path "*/env" -prune -o \
                -path "*/.env" -prune -o \
                -path "*/dist" -prune -o \
                -path "*/build" -prune -o \
                -path "*/target" -prune -o \
                -path "*/cache" -prune -o \
                -path "*/tmp" -prune -o \
                -path "*/temp" -prune -o \
                -maxdepth 5 \
                \( -name "AmazonQ.md" -o -name "CLAUDE.md" \) \
                -type f -print 2>/dev/null
        }
        
        # Search common directories that exist
        for dir in "${common_dirs[@]}"; do
            if [ -d "$dir" ]; then
                while IFS= read -r ai_file; do
                    repo_dir=$(dirname "$ai_file")
                    # Check if it's a git repo and not already in our list
                    if [ -d "$repo_dir/.git" ] && [[ ! " ${repos[@]} " =~ " ${repo_dir} " ]]; then
                        repos+=("$repo_dir")
                    fi
                done < <(search_directory "$dir")
            fi
        done
        
        echo "Found ${#repos[@]} repositories with AI context files"
        echo
        
        if [ ${#repos[@]} -eq 0 ]; then
            echo "No repositories found with AI context files"
            exit 0
        fi
        
        # Show what will be processed
        echo "Will process:"
        for repo in "${repos[@]}"; do
            echo "  - $repo"
        done
        echo
        
        read -p "Continue? (y/N) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            echo "Aborted."
            exit 0
        fi
    else
        # Use provided paths
        repos=("$@")
    fi
    
    echo
    
    # Process each repository
    local success=0
    local failed=0
    
    # Check if we can use parallel processing
    if command -v xargs &>/dev/null && [ ${#repos[@]} -gt 3 ]; then
        log_info "Processing ${#repos[@]} repositories in parallel..."
        
        # Export the function so xargs can use it
        export -f process_repo
        export -f log_info log_success log_warning log_error
        
        # Source logging.sh in subshells
        export LOGGING_SOURCE
        if [[ -f ~/bin/logging.sh ]]; then
            LOGGING_SOURCE="source ~/bin/logging.sh"
        else
            LOGGING_SOURCE="source ${SCRIPT_DIR}/../utils/logging.sh"
        fi
        
        # Process in parallel with max 4 jobs
        printf '%s\n' "${repos[@]}" | xargs -P 4 -I {} bash -c "$LOGGING_SOURCE; process_repo \"\$1\"" _ {} 2>&1
        
        # Can't easily track success/failed in parallel mode
        success=${#repos[@]}
        failed=0
    else
        # Fall back to sequential processing
        for repo in "${repos[@]}"; do
            if process_repo "$repo"; then
                ((success++))
            else
                ((failed++))
            fi
            echo
        done
    fi
    
    # Summary
    echo "Summary:"
    echo "  âœ“ Successful: $success"
    if [ $failed -gt 0 ]; then
        echo "  âœ— Failed: $failed"
    fi
}

# Run main function
main "$@"